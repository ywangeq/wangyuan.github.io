---
layout:     post
title:      Reading
subtitle:   Non-local Neural Network
date:       2020-01-02
author:     WY
header-img: img/post-bg-debug.jpg
catalog: true
tags:
    - Readiing
    - Semantic Segmentation
    - Non-local neural
---
> 一次面试的时候，由于博主是着重关注于语义分割这个领域，被问到了non-local neural network的优缺点

文章的motivation
> 由于convolution和recurrent 都是对图像的局部区域进行操作，所有是一类典型的local operation
> 而本文提出的method 尝试着引入非局部均值，来建立图像上两个有一定距离的像素之间的联系，实际上这就是一种空间信息的利用

为了增加卷积层的感受野，我们一般是通过堆叠卷积层，或者池化来解决，但是这样计算量和复杂度都会增加

**其他方法**
- non local module
- 空洞卷积核
- 类似PSPnet的预测attention 来集合上下文信息

**non local nueral Network**
缺点
- 当feature map 较大时， 考虑全局感受视野势必造成cost的增加
- 只考虑的spatial的关系，没有考虑各个channel之间的信息

之后的改进：
- criss-cross attention
- patch-non-local module


**criss-cross attention**
> 作者采用当前位置 一区同行同列的像素的相关性得到水平方向和垂直方向的远距离像素的上下文信息
> 并且叠加两个相同模块每个像素的感受视野为全局平局，大大减少了运算量
![cc](https://raw.githubusercontent.com/ywangeq/ywangeq.github.io/master/img/cc_attention.png)

（a） non-local 可以看到attention map为H x W, 右边红色特征图表示含有self-attention的feature map
（b） cc_attention 中间的attention为Hx (H+W-1)
因此 原本需要计算（HxW）^2次，现在变成了（H \cdot w) \cdot (H+W-1)