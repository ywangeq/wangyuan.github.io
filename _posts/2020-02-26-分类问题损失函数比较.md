---
layout:     post
title:      机器学习
subtitle:   分类问题(损失函数，正则化)
date:       2020-02-26
author:     WY
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - 损失函数
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>


损失函数的一般表示为$L(y,f(s))$,用以衡量真实值y和预测值$f(x)$之间不一致的程度，一般越小越接近，为了便于不同损失函数的比较，常将其表示为单变量的函数，在回归问题中这个变量为$y-f(x)$
在分类问题中为$yf(x)$

#### 回归问题的损失函数

回归问题中的$y$和$f(x)$皆为实数，因此用$y-f(x)$来度量二者的不一致程度。残差的绝对值越大，则损失函数越大，学习出来的模型效果就越差（这里不考虑正则化问题）

平方损失(squared loss): $(y-f(x))^2$
绝对值(absolute loss): $|y-f(x)|$
$$Huber损失(huber loss) f(x)=
                \begin{cases}
                \frac{1}{2}*|y-f(x)|^2& |y-f(x)|<=\omega\\
                \omega*|y-f(x)|-\frac{1}{2}*\ omega^2& |y-f(x)|>\omega
                \end{cases}$$
                

最常用的是平方损失，缺点是对于异常点会施加较大的惩罚，因而不够robust，如果有较多的异常点，则绝对值损失表现会比较好，但是绝对值损失的缺点是在y-f(x)=0的地方不可导，不容易优化
Huber是两者的综合，对于两者的缺点有一定程度的改善

#### 分类问题的损失函数

对于一个二分类问题，$y \in {-1,+1}$ ,损失函数常表示为关于yf(x)的单调递减形式

通常我们把yf(x) 称为margin，其作用类似于回归问题中的残差

$$二分类问题中的分类规则通常为sign(f(x))=
\begin{cases}
+1 & if & f(x)>=0 \\
-1 &  if &f(x)<0
\end{cases}$$

从通项中我们可以看到，如果$yf(x)>0$,则样本分类正确，$yf(x)$<0 则分类错误，而相应的分类决策边界即为f(x)=0,所以最小化损失函数也可以看成最大化margin的过程。
任何合格的分类损失函数都应该对margin<0的样本施加较大的惩罚

##### 1 0-1损失（zero-one loss)

$$L(y,f(x))=
\begin{cases}
0 & if & yf(x)>=0 \\
1 & if & yf(x)<0
\end{cases}
$$

0-1 损失对每个错分类点都施加相同的惩罚，这样那些（margin->-inf)的点并不会收到太大的关注，而且0-1损失本身不连续，非凸，优化困难，因而尝试用其他代理的损失函数进行优化


##### Logitic loss

$L(y,f(x))=log(1+e^{-yf(x)})$
logistic loss 为logistic regression中使用的损失函数，证明如下
我们已知Logistic Regression 中使用了Sigmoid函数表述预测概率:
$g(f(x)) = P(y=1|x) = \frac{1}{1+e^{-f(x)}}$
而 $P(y=-1|x) = 1-P(y=1|x)=1-\frac{1}{1+e^{-f(x)}}=\frac{1}{1+e^{f(x)}}=g(-f(x))$

因此利于 $y \in {-1,+1}$
我们可以得到 P(y|x) = $\frac{1}{1+e^{-y*f(x)}}$,这个为概率模型
接着我们通过极大似然的思想：

$max(\prod_{i=1}^{m}P(y_i|x_i))=max(\prod_{i=1}^{m}\frac{1}{1+e^{-y_if(x_i)}})$
同时对两边取对数，因为是求损失韩式，则将极大转为极小:
$max(\sum_{i=1}^{m}\log{P(y_i|x_i)})$ = $-min(\sum_{i=1}^{m}\log\frac{1}{1+e^{-yf(x)}})$= $min(\sum_{i=1}^{m}\log{1+e^{-yf(x)}})$ 
这样就是logistic loss

如果我们定义$t=\frac{y+1}{2} \in {0,1}$,则可转变为

$\prod_{i=1}^{m}{P(t_i=1|x_i)^{t_i}*(1-P(t_i=1|x_i))^{1-t_i}}$
不难看出这个就是corss entropy loss， 在二分类问题中logistic loss 和交叉熵损失是等价的。

**补充** 逻辑回归简单来说是一种用于解决二分类问题的机器学习方法，用于估计某种事情发生的状况。(直接返回结果，不返回概率)

##### L1,L2正则化
L1，L2正则化众所周知一个是绝对值之和，一个是平方和开根号
我们可以简单的理解，L2正则是尝试用一个圆去逼近目标，而L1正则相当于用菱形去逼近目标，因此L1容易一起稀疏解

1. 从导数的角度解释：
   L2正则无法将目标函数的极值点拉拢到稀疏解上，而L1正则因为L1导数的特殊性可以在一定范围能将极值点拉拢到稀疏解
   也就是说在梯度下降过程中L1正则给与了更大的下降力度，从而更快的收敛的稀疏点上

2. 从先验概率分布角度解释：
    L2正则相当与假设样本是服从高斯分布的，而L1正则相当于将设样本是服从拉普拉斯分布的。