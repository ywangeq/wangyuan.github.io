---
layout:     post
title:      Reading
subtitle:   PointRend: Image Segmentation as Rendering
date:       2020-02-08
author:     WY
header-img: img/post-bg-debug.jpg
catalog: true
tags:
    - Reading
    - Semantic Segmentation
    - Context 
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

- 先看了一遍文章，本篇何大神主要是设计了一个module来解决分割任务中边缘不够精细的问题。
- 以MaskRCNN为例子，受限于计算量和显存，对于每个ROIAlign之后的候选域，只会upsample到28*28后输出mask。
那么如何在不消耗太多代价的情况下仍然得到精细的分割结果呢，有一个经验是，我们会发现类似CRF这些喜欢去优化物体的边缘，其实不难发现，
语义分割很多不准确的部分都在物体的边缘，而且这些边缘只占了整个物体中很小的一部分。所以基于这个想法，作者提出了一个module在预测出来的mask中搜寻N个不确定的位置经行更加细致的分割预测。
![module](https://raw.githubusercontent.com/ywangeq/ywangeq.github.io/master/img/Pointrend_module.png)

如上图所示，在encoder完成时，获得的4x4的map，上采样到8x8,然后基于8x8的特征去推断一系列候选点，再进行细致的判断。
文中说每个细分点的特征是通过二插值来得到的，而每个位置上的classifier是通过简单的MLP来预测
如同baseline所示
![baseline](https://raw.githubusercontent.com/ywangeq/ywangeq.github.io/master/img/pointrend_baseline.png)

而候选点采样的话，作者做了不少实验，用两个参数去优化
如图
![sample](https://raw.githubusercontent.com/ywangeq/ywangeq.github.io/master/img/Pointrend_sample.png)

通过这个方法，能够很突出的优化边缘细节，而且不像后处理的哪种方法，需要很多的开销
一些结果如图
![result](https://raw.githubusercontent.com/ywangeq/ywangeq.github.io/master/img/point_resr.png)

