---
layout:     post
title:      深度学习
subtitle:   BN,softmax
date:       2020-02-27
author:     WY
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - 基本考点
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

#### Batch Normalization 

首先我们要知道一个概念就是独立同分布假设，假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能个在测试集上获得好效果的一个基本保障。
而BatchNorm就是为了在深度神经网络训练过程中是每一层神经网络输入保持相同分布

1. Internal Covarite shift
Bn正是用来解决这个概念的， 我们知道Mini-Batch SGD相对于One Example SGD的两个优势：梯度更新方向更准确，并行计算速度更快
什么是**covariate shift**呢如果实例集合<X,Y>中的输入值X的分布老是变化，纳闷对于网络模型来说就很难学习，也就是说在训练过程中，因为dp的各层参数在不停变化，所以每个隐藏层都会面临covariate shift的问题
因此BN的思想就是，为了避免inbernal covarite shift能不能让每个隐含层节点的**激活输入分布**固定下来

2. 为什么训练深层神经网络收敛越来越慢

这一点我们可以简单个看器非线性变换过程，深层神经网络在做非线性变换前的**激活输入**(就是x=wu+b)，u认为是输入，随着网络深度的加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是由于整体分布逐渐往非线性函数的取值区间的上下限两端靠近。
例如Sigmoid,意味着激活输入值WU+B的最大复制或正值，所以在反向传播时，低层神经网络的梯度会消失。

而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布拉回到均值为0，方差为1的标准正态分布，便面梯度消失问题的产生。
但是单单这么做是会影响网络的表达能力的，所以为了保证BN非线性的获得，对于变换后的满足均值0方差为1的x又进行了scale和shift的操作，每个神经元增加了两个参数scale和shift参数，这两个参数学习等到
**核心思想**是找到一个线性和非线性较好的平衡点，既可以享受非线性的强表达能力，又避免网络收敛的太慢

算法流程，**首先我们要确定**取均值过程的这个mean是通过batch得到的

**Input** : Values of x over a mini-batch: $B={x_1....m}$
            Parameters to be learned:$\alpha,\beta$

**Output**: {$y_i=BN_{\alpha,\beta(x_i)}$}

**mini-batch mean** : $\mu_{B} = \frac{1}{m}*\sum_{i=i}{m}x_i$

**mini-batch variance** : $\sigma^{2}_{B}=\frac{1}{m} \sum_{i=1} m(x_i-\mu_{B})^2$

**normalize** $\bar{x}=\frac{x_i-\mu_{B}}{\sqrt{\sigma^{2}_{B}+\varepsilon}}$

**scale and shift** $y_i = \alpha\bar{x_i}+\beta$

3. BatchNorm的inference过程

我们可以发现BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活数值调整，但是在推理(inference)的过程中，只有一个实例，这样是没办法算出需要的实例集合的方差和均值的。
既然没有从Mini-Batch数据里可以得到的统计量，我们可以从所有训练实例中获得的统计量来替代mini-batch里面m个训练实例获得的均值和方法，因此在推理的时候直接全局统计计量即可。
决定了获得统计量的数据范围，那么接下来的如何获得均值和方差呢。很简单，每次做mini-batch训练时，都会记录下mini-batch中m个训练实例获得的均值和方差，现在要全局统计量，只要把每个mini-bach的均值和方差计量记住，然后对这些均值和方差求数学期望

$E(x) = E_B(\mu_{B})$
$Var(x) = \frac{m}{m-1}E(\sigma^2_{B})$
有了均值和方差，每个hidden layer也已经有对应训练好的scaling参数和shift，也就可以推理的时候对每个神经激活数据进行bn计算了，公式很类似
$y= \frac{\sigma}{\sqrt{Var(x)+\varepsilon}}*x + (\beta-\frac{\sigma*E(x)}{\sqrt{Var(x)+\varepsilon}})$

4. 好处
- 1.极大提升了训练速度，收敛剁成大大加快
- 2.提升分类效果过，类似与引入了一种正则化表达式，防止过拟合
- 3.学习率可以加大，


#### Softmax函数以及求导

最为分类任务的输出层，我们可以认为softmax是几个类别选择的概率，并且使得概率和为1
$a_i=\frac{e^{z_i}}{\sum_{k}e^{z_k}}$
其中$z_i$代表神经元的输出
$z_i =\sum_{j}w_{ij}x_{ij}+b$，其中$w_{ij}是第i个神经元的第j个权重，b是偏移值$，$z_i$表示该网络的第i个输出

现在我们给出**cross entropy loss**: $C=-\sum_i*y_i*lna_i$

求导过程：
loss 对于神经元输出($z_i$)的梯度: $\frac{\partial{C}}{\partial{z_i}}$
根据复合函数求导法则：
$\frac{\partial{C}}{\partial{z_i}}=\frac{\partial{C}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{z_i}}$
注意 这边是$a_j$ 而不是$a_i$，softmax的公式里因为softmax公式的特性，他的分母包含了所有神经元的输出，所以有两种情况求导

我们先分开看
其中一个导数：
$\frac{\partial{C}}{\partial{a_j}}=\frac{\partial(-\sum_{j}(y_j*lna_j))}{\partial{a_j}}=-\sum_{j}(y_j)*(\frac{1}{a_j})$

另一个导数
i=j
$\frac{\partial{a_i}}{\partial{z_i}}$=$\frac{\partial{\frac{e^{z_i}}{\sum_k(e^{z_k})}}}{\partial{z_i}}$=$\frac{\sum_k{e^{z_k}*e^{z_i}}-(e^{z_i})^2}{\sum_k{(e^{z_k})^2}}$=$(\frac{e^{z_i}}{\sum_k{e^{z_k}}})(1-\frac{e^{z_i}}{\sum_k{e^{z_k}}})$ = $a_i(1-a_i)$
i != j
$\frac{\partial{a_j}}{\partial{z_i}}$=$\frac{\partial{\frac{e^{z_j}}{\sum_k(e^{z_k})}}}{\partial{z_i}}$=$-e^{z_j}(\frac{1}{\sum_k{e^{z_k}}})^2(e^z_i)$=$-a_{i}a_j$

#### 卷积操作对feature shape的影响
经过cnn， 一个长为7x7，kernel 3x3 stride=1的输出map = （7-3+2*0）/stride +1 =5
W2 =(W1-K +2P)/Stride +1
如果有空洞卷 W = (W1+2P -d(k-1)-1)/stride +1
parameter  input channel =C   ， (Kernel*kernel*C_in)* Cout